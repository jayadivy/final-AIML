{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc604a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcd2947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b8caef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1123757263427186690</td>\n",
       "      <td>hate wen females hit ah nigga with tht bro üòÇüòÇ,...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1123733301397733380</td>\n",
       "      <td>RT @airjunebug: When you're from the Bay but y...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1123734094108659712</td>\n",
       "      <td>RT @DonaldJTrumpJr: Dear Democrats: The Americ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1126951188170199049</td>\n",
       "      <td>RT @SheLoveTimothy: He ain‚Äôt on drugs he just ...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1126863510447710208</td>\n",
       "      <td>RT @TavianJordan: Summer ‚Äò19 I‚Äôm coming for yo...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1126798721025544193</td>\n",
       "      <td>RT @prodnose: Good morning, everyone.\\nFollowi...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1126833089190219777</td>\n",
       "      <td>@cheezitking123 this what you get for tryna ge...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1130037092845670400</td>\n",
       "      <td>earphones ko üò≠üò≠üò≠üò≠üò≠üò≠üò≠</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1127028455651123201</td>\n",
       "      <td>RT @nj_linguist: @realgonegirl @elivalley I th...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1130285076858789889</td>\n",
       "      <td>i‚Äôm tired as fuck. and man, physically ain‚Äôt S...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id                                               text  \\\n",
       "0    1123757263427186690  hate wen females hit ah nigga with tht bro üòÇüòÇ,...   \n",
       "1    1123733301397733380  RT @airjunebug: When you're from the Bay but y...   \n",
       "2    1123734094108659712  RT @DonaldJTrumpJr: Dear Democrats: The Americ...   \n",
       "3    1126951188170199049  RT @SheLoveTimothy: He ain‚Äôt on drugs he just ...   \n",
       "4    1126863510447710208  RT @TavianJordan: Summer ‚Äò19 I‚Äôm coming for yo...   \n",
       "..                   ...                                                ...   \n",
       "995  1126798721025544193  RT @prodnose: Good morning, everyone.\\nFollowi...   \n",
       "996  1126833089190219777  @cheezitking123 this what you get for tryna ge...   \n",
       "997  1130037092845670400                               earphones ko üò≠üò≠üò≠üò≠üò≠üò≠üò≠   \n",
       "998  1127028455651123201  RT @nj_linguist: @realgonegirl @elivalley I th...   \n",
       "999  1130285076858789889  i‚Äôm tired as fuck. and man, physically ain‚Äôt S...   \n",
       "\n",
       "    task1  \n",
       "0     HOF  \n",
       "1     HOF  \n",
       "2     NOT  \n",
       "3     HOF  \n",
       "4     NOT  \n",
       "..    ...  \n",
       "995   NOT  \n",
       "996   NOT  \n",
       "997   NOT  \n",
       "998   NOT  \n",
       "999   HOF  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('sample_text - Sheet1 (1).csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c6b872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emot\n",
      "  Downloading emot-3.1-py3-none-any.whl (61 kB)\n",
      "     -------------------------------------- 61.5/61.5 kB 648.3 kB/s eta 0:00:00\n",
      "Installing collected packages: emot\n",
      "Successfully installed emot-3.1\n",
      "Collecting emoji\n",
      "  Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n",
      "     -------------------------------------- 397.5/397.5 kB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.9.0\n",
      "Collecting demoji\n",
      "  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 42.9/42.9 kB 1.1 MB/s eta 0:00:00\n",
      "Installing collected packages: demoji\n",
      "Successfully installed demoji-1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement punct (from versions: none)\n",
      "ERROR: No matching distribution found for punct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "     -------------------------------------- 289.9/289.9 kB 1.2 MB/s eta 0:00:00\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-2.0.0-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install emot\n",
    "!pip install emoji\n",
    "!pip install demoji\n",
    "!pip install punct\n",
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc77da54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "255aabce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e42e162a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_8780\\1435932555.py:23: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    }
   ],
   "source": [
    "#from textblob import TextBlob\n",
    "import emot\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import emoji\n",
    "import demoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import regex\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "demoji.download_codes()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "226964c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1123757263427186690</td>\n",
       "      <td>hate wen females hit ah nigga with tht bro üòÇüòÇ,...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1123733301397733380</td>\n",
       "      <td>RT @airjunebug: When you're from the Bay but y...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1123734094108659712</td>\n",
       "      <td>RT @DonaldJTrumpJr: Dear Democrats: The Americ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1126951188170199049</td>\n",
       "      <td>RT @SheLoveTimothy: He ain‚Äôt on drugs he just ...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1126863510447710208</td>\n",
       "      <td>RT @TavianJordan: Summer ‚Äò19 I‚Äôm coming for yo...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1123757263427186690  hate wen females hit ah nigga with tht bro üòÇüòÇ,...   \n",
       "1  1123733301397733380  RT @airjunebug: When you're from the Bay but y...   \n",
       "2  1123734094108659712  RT @DonaldJTrumpJr: Dear Democrats: The Americ...   \n",
       "3  1126951188170199049  RT @SheLoveTimothy: He ain‚Äôt on drugs he just ...   \n",
       "4  1126863510447710208  RT @TavianJordan: Summer ‚Äò19 I‚Äôm coming for yo...   \n",
       "\n",
       "  task1  \n",
       "0   HOF  \n",
       "1   HOF  \n",
       "2   NOT  \n",
       "3   HOF  \n",
       "4   NOT  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=data['text']\n",
    "y=data['task1']\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1941918",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ff93d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253    RT @YoungTheGoat: Nigga Said ‚ÄúWyd‚Äù niggas are ...\n",
       "667    RT @Saintsfan5348: @RepJimLower @realDonaldTru...\n",
       "85     @CHIMPSINSOCKS And all the other complete and ...\n",
       "969    Bitches be pressed as fuuuuuck!!!!! You don't ...\n",
       "75     RT @bilalfqi: While Shias are protesting again...\n",
       "                             ...                        \n",
       "835    Been a while since I had a little Willie in my...\n",
       "192    RT @OgbeniDipo: To my young followers, please ...\n",
       "629                 @wolfrad_hentai That‚Äôs a lotta ass üëÄ\n",
       "559    RT @ggukreum: GET THE DAMN GETTY IMAGES WATERM...\n",
       "684    RT @uthman_waxcav: You're making mouth all ove...\n",
       "Name: text, Length: 750, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ebf29ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "993    RT @OfficialChip: There will never be another ...\n",
       "859    RT @taeilrnb: flo taeil shit, bitch ! https://...\n",
       "298    @MiguelSantosIII @tamwelsh Yes. Unfortunately ...\n",
       "553    RT @GGYounggBoy: I rather be single then stupi...\n",
       "672    Not getting any sleep tonight, will be time fo...\n",
       "                             ...                        \n",
       "462    RT @frankwarren_tv: ‚ÄúCongratulations @BronzeBo...\n",
       "356    RT @philsadelphia: in honor of paul rudd hosti...\n",
       "2      RT @DonaldJTrumpJr: Dear Democrats: The Americ...\n",
       "478    RT @simpboylen: might fuck around and get myse...\n",
       "695    When the AirBnB has banging mirrors all round ...\n",
       "Name: text, Length: 250, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0633b67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253    HOF\n",
       "667    HOF\n",
       "85     HOF\n",
       "969    HOF\n",
       "75     NOT\n",
       "      ... \n",
       "835    NOT\n",
       "192    NOT\n",
       "629    HOF\n",
       "559    HOF\n",
       "684    NOT\n",
       "Name: task1, Length: 750, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10b97bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "993    HOF\n",
       "859    HOF\n",
       "298    HOF\n",
       "553    HOF\n",
       "672    NOT\n",
       "      ... \n",
       "462    NOT\n",
       "356    NOT\n",
       "2      NOT\n",
       "478    HOF\n",
       "695    HOF\n",
       "Name: task1, Length: 250, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "470269fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HOF    501\n",
       "NOT    499\n",
       "Name: task1, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['task1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7f6a9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c39501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emot_object = emot.core.emot()\n",
    "ps =PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "english_stopwords = stopwords.words('english')\n",
    "exclude = set(string.punctuation)\n",
    "def preprocess(text):\n",
    "  #text=demoji.findall(df['Text'])\n",
    "    text = contractions.fix(text.lower(), slang=True)\n",
    "    text =re.sub(\"@ ?[A-Za-z0-9_]+\", \"\", text)\n",
    "    text= re.sub(r'\\d+', '', text)\n",
    "    text=re.sub(r'$', '', text)\n",
    "    text= re.sub(r'‚Äô','', text )\n",
    "    text=re.sub('<.*?>','',text)\n",
    "    text=re.sub(r'http\\S+', '', text)\n",
    "  #text=emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    text = ''.join(ch for ch in text if ch not in exclude)\n",
    "    tokens = word_tokenize(text)\n",
    "  #print(\"Tokens:\", tokens)\n",
    "    text = [t for t in tokens if t not in english_stopwords]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e27b53b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "#import demoji\n",
    "#demoji.download_codes()\n",
    "def emo(text):\n",
    "    \n",
    "    temp=emoji.demojize(text,delimiters=(\" \",\" \"))\n",
    "    temp=temp.replace(\"_\",\"  \")\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efaa3309",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text']=data[\"text\"].apply(lambda x:emo(x))\n",
    "data[\"clean_text\"]=data['clean_text'].apply(lambda X: preprocess(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d85ddb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1123757263427186690</td>\n",
       "      <td>hate wen females hit ah nigga with tht bro üòÇüòÇ,...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>hate wen females hit ah nigga tht bro face tea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1123733301397733380</td>\n",
       "      <td>RT @airjunebug: When you're from the Bay but y...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>rt bay really ny nigga heart w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1123734094108659712</td>\n",
       "      <td>RT @DonaldJTrumpJr: Dear Democrats: The Americ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>rt dear democrats american people stupid know ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1126951188170199049</td>\n",
       "      <td>RT @SheLoveTimothy: He ain‚Äôt on drugs he just ...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>rt drugs bored shit bored face tears joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1126863510447710208</td>\n",
       "      <td>RT @TavianJordan: Summer ‚Äò19 I‚Äôm coming for yo...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>rt summer ‚Äò coming boring shit beach days road...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1126798721025544193</td>\n",
       "      <td>RT @prodnose: Good morning, everyone.\\nFollowi...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>rt good morning everyone following one worst d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1126833089190219777</td>\n",
       "      <td>@cheezitking123 this what you get for tryna ge...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>get tryna get kfc expressionless face expressi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1130037092845670400</td>\n",
       "      <td>earphones ko üò≠üò≠üò≠üò≠üò≠üò≠üò≠</td>\n",
       "      <td>NOT</td>\n",
       "      <td>earphones ko loudly crying face loudly crying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1127028455651123201</td>\n",
       "      <td>RT @nj_linguist: @realgonegirl @elivalley I th...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>rt linguist think people need realize art ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1130285076858789889</td>\n",
       "      <td>i‚Äôm tired as fuck. and man, physically ain‚Äôt S...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>tired fuck man physically shit mentally draine...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id                                               text  \\\n",
       "0    1123757263427186690  hate wen females hit ah nigga with tht bro üòÇüòÇ,...   \n",
       "1    1123733301397733380  RT @airjunebug: When you're from the Bay but y...   \n",
       "2    1123734094108659712  RT @DonaldJTrumpJr: Dear Democrats: The Americ...   \n",
       "3    1126951188170199049  RT @SheLoveTimothy: He ain‚Äôt on drugs he just ...   \n",
       "4    1126863510447710208  RT @TavianJordan: Summer ‚Äò19 I‚Äôm coming for yo...   \n",
       "..                   ...                                                ...   \n",
       "995  1126798721025544193  RT @prodnose: Good morning, everyone.\\nFollowi...   \n",
       "996  1126833089190219777  @cheezitking123 this what you get for tryna ge...   \n",
       "997  1130037092845670400                               earphones ko üò≠üò≠üò≠üò≠üò≠üò≠üò≠   \n",
       "998  1127028455651123201  RT @nj_linguist: @realgonegirl @elivalley I th...   \n",
       "999  1130285076858789889  i‚Äôm tired as fuck. and man, physically ain‚Äôt S...   \n",
       "\n",
       "    task1                                         clean_text  \n",
       "0     HOF  hate wen females hit ah nigga tht bro face tea...  \n",
       "1     HOF                     rt bay really ny nigga heart w  \n",
       "2     NOT  rt dear democrats american people stupid know ...  \n",
       "3     HOF           rt drugs bored shit bored face tears joy  \n",
       "4     NOT  rt summer ‚Äò coming boring shit beach days road...  \n",
       "..    ...                                                ...  \n",
       "995   NOT  rt good morning everyone following one worst d...  \n",
       "996   NOT  get tryna get kfc expressionless face expressi...  \n",
       "997   NOT  earphones ko loudly crying face loudly crying ...  \n",
       "998   NOT  rt linguist think people need realize art ever...  \n",
       "999   HOF  tired fuck man physically shit mentally draine...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b0c5ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import regex\n",
    "\n",
    "def custom_analyzer(text):\n",
    "    words = regex.findall(r'\\w{2,}', text) # extract words of at least 2 letters\n",
    "    for w in words:\n",
    "        yield w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b226041e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1123757263427186690</td>\n",
       "      <td>hate wen females hit ah nigga with tht bro üòÇüòÇ,...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>hate wen females hit ah nigga tht bro face tea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1123733301397733380</td>\n",
       "      <td>RT @airjunebug: When you're from the Bay but y...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>rt bay really ny nigga heart w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1123734094108659712</td>\n",
       "      <td>RT @DonaldJTrumpJr: Dear Democrats: The Americ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>rt dear democrats american people stupid know ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1126951188170199049</td>\n",
       "      <td>RT @SheLoveTimothy: He ain‚Äôt on drugs he just ...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>rt drugs bored shit bored face tears joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1126863510447710208</td>\n",
       "      <td>RT @TavianJordan: Summer ‚Äò19 I‚Äôm coming for yo...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>rt summer ‚Äò coming boring shit beach days road...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1126798721025544193</td>\n",
       "      <td>RT @prodnose: Good morning, everyone.\\nFollowi...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>rt good morning everyone following one worst d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1126833089190219777</td>\n",
       "      <td>@cheezitking123 this what you get for tryna ge...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>get tryna get kfc expressionless face expressi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1130037092845670400</td>\n",
       "      <td>earphones ko üò≠üò≠üò≠üò≠üò≠üò≠üò≠</td>\n",
       "      <td>NOT</td>\n",
       "      <td>earphones ko loudly crying face loudly crying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1127028455651123201</td>\n",
       "      <td>RT @nj_linguist: @realgonegirl @elivalley I th...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>rt linguist think people need realize art ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1130285076858789889</td>\n",
       "      <td>i‚Äôm tired as fuck. and man, physically ain‚Äôt S...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>tired fuck man physically shit mentally draine...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id                                               text  \\\n",
       "0    1123757263427186690  hate wen females hit ah nigga with tht bro üòÇüòÇ,...   \n",
       "1    1123733301397733380  RT @airjunebug: When you're from the Bay but y...   \n",
       "2    1123734094108659712  RT @DonaldJTrumpJr: Dear Democrats: The Americ...   \n",
       "3    1126951188170199049  RT @SheLoveTimothy: He ain‚Äôt on drugs he just ...   \n",
       "4    1126863510447710208  RT @TavianJordan: Summer ‚Äò19 I‚Äôm coming for yo...   \n",
       "..                   ...                                                ...   \n",
       "995  1126798721025544193  RT @prodnose: Good morning, everyone.\\nFollowi...   \n",
       "996  1126833089190219777  @cheezitking123 this what you get for tryna ge...   \n",
       "997  1130037092845670400                               earphones ko üò≠üò≠üò≠üò≠üò≠üò≠üò≠   \n",
       "998  1127028455651123201  RT @nj_linguist: @realgonegirl @elivalley I th...   \n",
       "999  1130285076858789889  i‚Äôm tired as fuck. and man, physically ain‚Äôt S...   \n",
       "\n",
       "    task1                                         clean_text  \n",
       "0     HOF  hate wen females hit ah nigga tht bro face tea...  \n",
       "1     HOF                     rt bay really ny nigga heart w  \n",
       "2     NOT  rt dear democrats american people stupid know ...  \n",
       "3     HOF           rt drugs bored shit bored face tears joy  \n",
       "4     NOT  rt summer ‚Äò coming boring shit beach days road...  \n",
       "..    ...                                                ...  \n",
       "995   NOT  rt good morning everyone following one worst d...  \n",
       "996   NOT  get tryna get kfc expressionless face expressi...  \n",
       "997   NOT  earphones ko loudly crying face loudly crying ...  \n",
       "998   NOT  rt linguist think people need realize art ever...  \n",
       "999   HOF  tired fuck man physically shit mentally draine...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97467e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without parameter passing......\n",
    "\n",
    "tf_idf = CountVectorizer()\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.fit_transform(data['clean_text'])\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.transform(data['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0acdb3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without parameter passing......\n",
    "\n",
    "tf_idf = CountVectorizer(analyzer='char', ngram_range=(2, 2))\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.fit_transform(data['clean_text'])\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.transform(data['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f50382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without parameter passing......\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.fit_transform(data['clean_text'])\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.transform(data['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "164ced58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with parameters\n",
    "\n",
    "tf_idf = TfidfVectorizer(analyzer='char',ngram_range=(1,3),max_df=1.0,min_df=1,max_features=5000)\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.fit_transform(data['clean_text'])\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.transform(data['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7203682",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer()\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.fit_transform(data['clean_text'])\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.transform(data['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2c7f64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x3482 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8428 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fec8194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming test data into tf-idf matrix\n",
    "X_test_tf = tf_idf.transform(data[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42d0d2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x3482 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8428 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13ad820b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "lsvc = LinearSVC()\n",
    "lsvc.fit(X_train_tf, data['task1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ca97817",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lsvc.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b95409a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF',\n",
       "       'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT',\n",
       "       'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF',\n",
       "       'NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF',\n",
       "       'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF',\n",
       "       'NOT', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'NOT',\n",
       "       'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT',\n",
       "       'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF',\n",
       "       'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF',\n",
       "       'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT',\n",
       "       'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT',\n",
       "       'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF',\n",
       "       'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF',\n",
       "       'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF',\n",
       "       'HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF',\n",
       "       'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF',\n",
       "       'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'NOT',\n",
       "       'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT',\n",
       "       'NOT', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT',\n",
       "       'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF',\n",
       "       'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'HOF',\n",
       "       'HOF', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT',\n",
       "       'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT',\n",
       "       'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF',\n",
       "       'NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'NOT',\n",
       "       'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF',\n",
       "       'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT',\n",
       "       'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF',\n",
       "       'HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF',\n",
       "       'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF',\n",
       "       'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF',\n",
       "       'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF',\n",
       "       'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF',\n",
       "       'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF',\n",
       "       'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF',\n",
       "       'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'NOT',\n",
       "       'HOF', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT',\n",
       "       'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'HOF',\n",
       "       'NOT', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF',\n",
       "       'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF',\n",
       "       'NOT', 'NOT', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF',\n",
       "       'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'NOT',\n",
       "       'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF',\n",
       "       'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT',\n",
       "       'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF',\n",
       "       'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'NOT',\n",
       "       'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'NOT',\n",
       "       'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT',\n",
       "       'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF',\n",
       "       'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF',\n",
       "       'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT',\n",
       "       'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF',\n",
       "       'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'NOT',\n",
       "       'HOF'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b4901d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "952d689b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      HOF\n",
       "1      HOF\n",
       "2      NOT\n",
       "3      HOF\n",
       "4      NOT\n",
       "      ... \n",
       "995    NOT\n",
       "996    NOT\n",
       "997    NOT\n",
       "998    NOT\n",
       "999    HOF\n",
       "Name: task1, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['task1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58a5cee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         HOF       0.99      1.00      1.00       501\n",
      "         NOT       1.00      0.99      1.00       499\n",
      "\n",
      "    accuracy                           1.00      1000\n",
      "   macro avg       1.00      1.00      1.00      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(data['task1'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5afe880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         HOF       0.97      0.99      0.98       501\n",
      "         NOT       0.99      0.96      0.98       499\n",
      "\n",
      "    accuracy                           0.98      1000\n",
      "   macro avg       0.98      0.98      0.98      1000\n",
      "weighted avg       0.98      0.98      0.98      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train_tf, data['task1'])\n",
    "naive_bayes_pred=naive_bayes_classifier.predict(X_test_tf)\n",
    "print(classification_report(data['task1'], naive_bayes_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a798d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d27eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
